{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# importing data manipulation as well as plotting packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# importing the environment canada data set with the date as the index column\n",
    "gwv_data = pd.read_csv('data/c46131.csv', sep=',', index_col='DATE')\n",
    "\n",
    "# Deleting empty columns\n",
    "del gwv_data['WSS$']\n",
    "del gwv_data['WSS$.1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zoo= pd.read_csv('data/Zooplankton_2009-2012.csv', sep=',', index_col='Date')\n",
    "zoo.index = pd.to_datetime(zoo.index, unit='d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gwv_data.index = pd.to_datetime(gwv_data.index, unit='m')\n",
    "\n",
    "    # Collapsing the data so that all data are averaged over a weekly perdiod\n",
    "gwv_weekly = gwv_data.resample('W', how=('mean'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Driven Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For my final project, I want to be able to create a series of timeseries plots that include my three variables of focus (zooplankton abundance, sea surface temperature, and significant wave height) for each year of the zooplnakton data (2009-2012). In order to make these plots in an organized fashion I need to pull the zooplankton data and the environmental data from each of thier respective dataframes for each spring (2009-2012). Instead of repeatedly using a code for each year to match both of my datatables' timeseries for plotting purposes, I think it would be easier to deveop a function that would do this automatically. I want to make this function generalized to perform for any two of my datasets (the combination of environmental data and spring bloom data). Ideally, function would convert each dataset into a datatable that would contain weekly averages of the data over the spring season for a desired year. Thus I am envisioning the function requiring 3 inputs, the name of my two datasets,  and the year of the spring desired by the user. \n",
    "\n",
    "To develop this function, I will use test driven development. Since this is predominantly a data munging funciton, I will test its functionality by indexing the data point in the 1st column and the 4th row. I compare this input in the observation and the expectation inside the test functions. For the functions acutal use however, I plan to make the it return the new dataframes for both datasets. Additionally, my function will assume that both datasets have a date based index. Below I will build my function piece by peice and use a new test each time to ensure its functionality. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first step of the function aims to resample any general dataset and average it over a week long period for any general dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def organize_1(dataset_1):\n",
    "    '''This function will average data per week for any general dataset'''\n",
    "    # Ensuring dataset_1 index (time) is in pandas datetime format\n",
    "    dataset_1.index = pd.to_datetime(dataset_1.index, unit='m')\n",
    "\n",
    "    # Resampling dataset_1 into weekly averages\n",
    "    dataset_1_weekly = dataset_1.resample('W', how=('mean'))\n",
    "    \n",
    "    # producing an output to test the function\n",
    "    return dataset_1_weekly.iloc[1,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_organize_1():\n",
    "    # Chagning the data index (time) into pandas datetime format\n",
    "    gwv_data.index = pd.to_datetime(gwv_data.index, unit='m')\n",
    "\n",
    "    # Collapsing the data so that all data are averaged over a weekly perdiod\n",
    "    gwv_weekly = gwv_data.resample('W', how=('mean'))\n",
    "    \n",
    "    # Testing my manual method of data resampling to the automated function\n",
    "    # I am indexing the 1st column and 4th row to compare these methods\n",
    "    expected = gwv_weekly.iloc[1,4]\n",
    "    obs = organize_1(gwv_data)\n",
    "    assert expected == obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_organize_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def organize_2(dataset_1, year):\n",
    "    '''This function will average data per week over the spring of 2010 for any general dataset'''\n",
    "    # Ensuring dataset_1 index (time) is in pandas datetime format\n",
    "    dataset_1.index = pd.to_datetime(dataset_1.index, unit='m')\n",
    "\n",
    "    # Resampling dataset_1 into weekly averages\n",
    "    dataset_1_weekly = dataset_1.resample('W', how=('mean'))\n",
    "    \n",
    "    # Subsetting the data for the spring plankton bloom of 2010 if year=2010 is specified\n",
    "    if year == 2010:\n",
    "        last = '2010-6-28'\n",
    "        before = '2010-03-03'\n",
    "    dataset_1_year_weekly = dataset_1_weekly[before:last]\n",
    "           \n",
    "    # producing an output to test the function    \n",
    "    return dataset_1_year_weekly.iloc[1,4]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_organize_2():\n",
    "    \n",
    "    # Chagning the gwv_data index (time) into pandas datetime format\n",
    "    gwv_data.index = pd.to_datetime(gwv_data.index, unit='m')\n",
    "\n",
    "    # Collapsing the gwv_data so that all data are averaged over a weekly perdiod\n",
    "    gwv_weekly = gwv_data.resample('W', how=('mean'))\n",
    "    \n",
    "    # Subsetting the gwv_data for the spring of 2010\n",
    "    gwv_year_weekly = gwv_weekly['2010-03-03':'2010-6-28']\n",
    "    \n",
    "    # Testing my manual method of data resampling to the automated function\n",
    "    # I am indexing the 1st column and 4th row to compare these methods\n",
    "    expected = gwv_year_weekly.iloc[1,4] \n",
    "    obs = organize_2(gwv_data, 2010)\n",
    "    assert expected == obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_organize_1()\n",
    "test_organize_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def organize_3(dataset_1, year):\n",
    "    '''This function will average data per week over the spring of 2010 or 2009 \n",
    "    for any general dataset'''\n",
    "    # Ensuring dataset_1 index (time) is in pandas datetime format\n",
    "    dataset_1.index = pd.to_datetime(dataset_1.index, unit='m')\n",
    "\n",
    "    # Resampling dataset_1 into weekly averages\n",
    "    dataset_1_weekly = dataset_1.resample('W', how=('mean'))\n",
    "    \n",
    "    # Subsetting the data for the spring plankton bloom of either 2010 or 2009 depending on the year specified\n",
    "    if year == 2010:\n",
    "        last = '2010-6-28'\n",
    "        before = '2010-03-03'\n",
    "    if year == 2009:\n",
    "        before ='2009-02-24'\n",
    "        last = '2009-07-05' \n",
    "    dataset_1_year_weekly = dataset_1_weekly[before:last]\n",
    "    \n",
    "    # producing an output to test the function\n",
    "    return dataset_1_year_weekly.iloc[1,4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_organize_3():\n",
    "    \n",
    "    # Chagning the data index (time) into pandas datetime format\n",
    "    gwv_data.index = pd.to_datetime(gwv_data.index, unit='m')\n",
    "\n",
    "    # Collapsing the data so that all data are averaged over a weekly perdiod\n",
    "    gwv_weekly = gwv_data.resample('W', how=('mean'))\n",
    "    \n",
    "    # Subsetting the gwv_data for the spring of 2009\n",
    "    gwv_year_weekly = gwv_weekly['2009-02-24':'2009-07-05' ]\n",
    "    \n",
    "    # Testing my manual method of data resampling to the automated function\n",
    "    # I am indexing the 1st column and 4th row to compare these methods\n",
    "    expected = gwv_year_weekly.iloc[1,4]\n",
    "    obs = organize_3(gwv_data, 2009)\n",
    "    assert expected == obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_organize_1()\n",
    "test_organize_2()\n",
    "test_organize_3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def organize_4(dataset_1, year):\n",
    "    '''This function will average data per week over the spring of 2012, 2011, 2010, \n",
    "    or 2009 for any general dataset'''\n",
    "    # Ensuring dataset_1 index (time) is in pandas datetime format\n",
    "    dataset_1.index = pd.to_datetime(dataset_1.index, unit='m')\n",
    "\n",
    "    # Resampling dataset_1 into weekly averages\n",
    "    dataset_1_weekly = dataset_1.resample('W', how=('mean'))\n",
    "    \n",
    "    # Subsetting the data for the spring plankton bloom of 2012, 2011, 2010 or 2009 depending on the year specified\n",
    "    if year == 2009:\n",
    "        first ='2009-02-24'\n",
    "        last = '2009-07-05' \n",
    "    if year == 2010:\n",
    "        first = '2010-03-03'\n",
    "        last = '2010-6-28'   \n",
    "    if year == 2011:\n",
    "        first = '2011-03-07'\n",
    "        last = '2011-06-30'\n",
    "    if year == 2012:\n",
    "        first ='2012-03-30'\n",
    "        last = '2012-06-25'\n",
    "    dataset_1_year_weekly = dataset_1_weekly[first:last]\n",
    "    \n",
    "    # producing an output to test the function\n",
    "    return dataset_1_year_weekly.iloc[1,4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_organize_4():\n",
    "     \n",
    "    # Chagning the data index (time) into pandas datetime format\n",
    "    gwv_data.index = pd.to_datetime(gwv_data.index, unit='m')\n",
    "\n",
    "    # Collapsing the data so that all data are averaged over a weekly perdiod\n",
    "    gwv_weekly = gwv_data.resample('W', how=('mean'))\n",
    "    \n",
    "     # Subsetting the gwv_data for the spring of 2011\n",
    "    gwv_year_weekly = gwv_weekly['2011-03-07':'2011-06-30']\n",
    "    \n",
    "    \n",
    "    # Testing my manual method of data resampling to the automated function\n",
    "    # I am indexing the 1st column and 4th row to compare these methods\n",
    "    expected = gwv_year_weekly.iloc[1,4]\n",
    "    obs = organize_4(gwv_data, 2011)\n",
    "    assert expected == obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_organize_1()\n",
    "test_organize_2()\n",
    "test_organize_3()\n",
    "test_organize_4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def organize_5(dataset_1, year, dataset_2):\n",
    "    '''This function will average data per week over the spring of 2012, 2011, 2010 or \n",
    "    2009 for my two datasets'''\n",
    "    # Ensuring dataset_1 index (time) is in pandas datetime format\n",
    "    dataset_1.index = pd.to_datetime(dataset_1.index, unit='m')\n",
    "\n",
    "    # Resampling dataset_1 into weekly averages\n",
    "    dataset_1_weekly = dataset_1.resample('W', how=('mean'))\n",
    "    \n",
    "    # Subsetting the dataset_1 for the spring plankton bloom in the 2010\n",
    "    if year == 2009:\n",
    "        first ='2009-02-24'\n",
    "        last = '2009-07-05' \n",
    "    if year == 2010:\n",
    "        first = '2010-03-03'\n",
    "        last = '2010-6-28'   \n",
    "    if year == 2011:\n",
    "        first = '2011-03-07'\n",
    "        last = '2011-06-30'\n",
    "    if year == 2012:\n",
    "        first ='2012-03-30'\n",
    "        last = '2012-06-25'\n",
    "    dataset_1_year_weekly = dataset_1_weekly[first:last]\n",
    "    \n",
    "    \n",
    "\n",
    "    # dropping the rows where naN's are present in dataset_1\n",
    "    dataset_1_year_weekly = dataset_1_year_weekly.dropna()\n",
    "    \n",
    "    # Ensuring dataset_2 index (time) in in pandas datetime format\n",
    "    zoo.index = pd.to_datetime(zoo.index, unit='d')\n",
    "    \n",
    "    # Now resampling dataset_2 into weekly averages over the desired spring\n",
    "    if year == 2009:\n",
    "        dataset_2_year = dataset_2['2009']\n",
    "    if year == 2010:\n",
    "        dataset_2_year = dataset_2['2010']\n",
    "    if year == 2011:\n",
    "        dataset_2_year = dataset_2['2011']\n",
    "    if year == 2012:\n",
    "        dataset_2_year = dataset_2['2012']\n",
    "    dataset_2_year_weekly = dataset_2_year.resample('W', how=('mean'))\n",
    "    \n",
    "    return dataset_2_year_weekly.iloc[1,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_organize_5():\n",
    "    # Resampling gwv_data (environment Canada data) into weekly averages \n",
    "    # Chagning the data index (time) into pandas datetime format\n",
    "    gwv_data.index = pd.to_datetime(gwv_data.index, unit='m')\n",
    "\n",
    "    # Collapsing the data so that all data are averaged over a weekly perdiod\n",
    "    gwv_weekly = gwv_data.resample('W', how=('mean'))\n",
    "    \n",
    "    # Subsetting the gwv_data for the spring of 2011\n",
    "    gwv_2011_weekly = gwv_weekly['2011-03-07':'2011-06-30']\n",
    "    \n",
    "    # Subsetting dataset_2 for the 2011 season\n",
    "    zoo_2011 = zoo['2011']\n",
    "    \n",
    "    # Resampling subset of dataset_2 into weekly averages\n",
    "    zoo_2011_weekly = zoo_2011.resample('W', how=('mean'))\n",
    "    \n",
    "    # Testing my manual method of data resampling to the automated function\n",
    "    # I am indexing the 1st column and 4th row to compare these methods\n",
    "    expected = zoo_2011_weekly.iloc[1,4] \n",
    "    obs = organize_5(gwv_data, 2011, zoo)\n",
    "    \n",
    "    assert expected == obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_organize_1()\n",
    "test_organize_2()\n",
    "test_organize_3()\n",
    "test_organize_4()\n",
    "test_organize_5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
